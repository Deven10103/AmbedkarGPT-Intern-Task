# AmbedkarGPT ‚Äî Local RAG Question‚ÄëAnswering System

## üìå About the Project
AmbedkarGPT is a **local Retrieval-Augmented Generation (RAG)** system that allows users to ask questions grounded strictly in a reference document.  
It loads text, splits it into chunks, builds a Chroma vector database, retrieves the most relevant chunks based on the query, and then generates an answer using a locally running LLM through **Ollama**.

### ‚úî What this project does
- Loads and processes `speech.txt`
- Builds a vector database using **ChromaDB**
- Retrieves the top‚Äëk most relevant chunks for any query
- Generates a **grounded** answer using a local LLM
- Automatically supports clean exit using commands like `quit` or `exit`

### ‚úî Technologies used
- **Python 3.10+**
- **ChromaDB** (vector store)
- **Sentence‚ÄëTransformers** (embeddings)
- **Ollama** (local LLM runner)
- **Mistral (via Ollama)** for text generation
- **Torch**, **Transformers**, **LangChain Community modules**

---

# Repository structure

```
AmbedkarGPT/
‚îÇ
‚îú‚îÄ‚îÄ main.py               # python code to create embeddings, Chroma vectorDB & answer generation (using Ollama)
‚îú‚îÄ‚îÄ speech.txt            # input document to index
‚îú‚îÄ‚îÄ chroma_db/            # generated vector DB folder
‚îú‚îÄ‚îÄ requirements.txt      # python dependencies
‚îî‚îÄ‚îÄ README.md
```
----

# üöÄ Installation & Usage Guide

Follow these steps to install and run the project locally.

---

## **1Ô∏è‚É£ Pull the repository**

```bash
git clone https://github.com/Deven10103/AmbedkarGPT-Intern-Task.git
cd AmbedkarGPT-Intern-Task
```

---

## **2Ô∏è‚É£ Create a virtual environment**

### Windows (PowerShell)
```powershell
python -m venv .venv
.venv\Scripts\activate.bat
```

### macOS / Linux
```bash
python3 -m venv .venv
source .venv/bin/activate
```

---

## **3Ô∏è‚É£ Install dependencies**

```bash
pip install -r requirements.txt
```

---

## **4Ô∏è‚É£ Install Ollama**
Download Ollama from the official site:  
**https://ollama.com/download**

---

## **5Ô∏è‚É£ Pull the Mistral model**

```bash
ollama pull mistral
```

---

## **6Ô∏è‚É£ Run the main script**

```bash
python main.py
```

---

## ‚úî Usage Instructions

After running the script, you will see a prompt like:

```
Enter your query:
```

- Type any question based on the document.
- If you want to exit the program, simply type:

```
quit
```
or
```
exit
```

---

# ‚ö†Ô∏è Important Note on GPU / VRAM Errors

If you face **model loading errors**, such as:

- `cudaMalloc failed`
- `failed to load model`
- `unable to allocate CUDA buffer`
- or any other GPU memory‚Äìrelated error

‚û° **Close all heavy applications**, especially:
- web browsers (Chrome / Edge / Firefox)
- VS Code / JetBrains tools
- games or GPU‚Äëaccelerated programs

These may be using GPU VRAM, preventing the model from loading.  
Closing them usually resolves the issue.

---

# Quick debug commands

```powershell
nvidia-smi             # check GPU VRAM & processes
ollama list            # list local models
ollama run mistral   # test model
```

---


If you need modifications or want to extend the README, feel free to ask!
